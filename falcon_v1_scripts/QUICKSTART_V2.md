# FALCON v2: Quick Start Guide

## ğŸš€ Setup Verified

All code has been tested and is working:
- âœ… AdamW baseline
- âœ… Muon hybrid (2D + AdamW)
- âœ… FALCON v2 with all optimizations

## ğŸ“ Files Created

```
falcon_v1_scripts/
â”œâ”€â”€ optim/
â”‚   â””â”€â”€ falcon.py                 # âœ¨ FALCON v2 (updated)
â”œâ”€â”€ train.py                       # âœ¨ Full experimental framework (updated)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_v2.sh                 # ğŸ†• Run all experiments
â”‚   â””â”€â”€ plot_results.py           # ğŸ†• Generate plots & tables
â”œâ”€â”€ FALCON_V2_SUMMARY.md          # ğŸ†• Implementation summary
â””â”€â”€ QUICKSTART_V2.md              # ğŸ†• This file
```

## âš¡ Quick Test (3 Epochs)

Test that everything works:

```bash
cd ~/projects/falcon_v1_scripts

# Test AdamW (~16s total)
python train.py --optimizer adamw --epochs 3 --exp test_adamw

# Test Muon (~18s total)
python train.py --optimizer muon --epochs 3 --exp test_muon

# Test FALCON v2 (~20s total)
python train.py --optimizer falcon --epochs 3 --exp test_falcon \
  --rank1-backend poweriter --mask-interval 5 --apply-stages "3,4" \
  --skip-mix-start 0.0 --skip-mix-end 0.7 \
  --retain-energy-start 0.90 --retain-energy-end 0.60
```

Expected 3-epoch results:
- AdamW: ~77% accuracy, ~5.4s/epoch
- Muon: ~74% accuracy, ~6.0s/epoch
- FALCON: ~77% accuracy, ~6.6s/epoch â† **Only 1.2x slower!**

## ğŸ§ª Run Full Experimental Suite

This will take ~8-10 hours total (11 runs Ã— 60 epochs each):

```bash
# Run all experiments
bash scripts/run_v2.sh

# Monitor progress (in another terminal)
tail -f runs/A1_full/metrics.csv
tail -f runs/F1_fast/metrics.csv

# When complete, generate plots
python scripts/plot_results.py

# View results
ls -lh results/
cat results/results.md
```

## ğŸ“Š Individual Experiments

If you want to run specific experiments:

### Primary Baselines (60 epochs each)

```bash
# AdamW baseline (~6 min)
python train.py --optimizer adamw --epochs 60 --exp A1_full

# Muon hybrid, LR=1.0 (~7 min)
python train.py --optimizer muon --epochs 60 --muon-lr-mult 1.00 --exp M1_full_lr100

# Muon hybrid, LR=1.25 (~7 min)
python train.py --optimizer muon --epochs 60 --muon-lr-mult 1.25 --exp M1_full_lr125

# FALCON v2 (fast settings) (~7-8 min)
python train.py --optimizer falcon --epochs 60 --exp F1_fast \
  --rank1-backend poweriter --poweriter-steps 1 --mask-interval 5 \
  --apply-stages "3,4" --skip-mix-start 0.0 --skip-mix-end 0.7 \
  --retain-energy-start 0.90 --retain-energy-end 0.60 --rank-k 1
```

### Robustness (High-Freq Noise)

```bash
# AdamW with noise
python train.py --optimizer adamw --epochs 60 --exp A1_noise \
  --test-highfreq-noise 0.15

# FALCON with noise
python train.py --optimizer falcon --epochs 60 --exp F1_noise \
  --test-highfreq-noise 0.15 \
  --rank1-backend poweriter --mask-interval 5 --apply-stages "3,4" \
  --skip-mix-start 0.0 --skip-mix-end 0.7 \
  --retain-energy-start 0.90 --retain-energy-end 0.60
```

### Data Efficiency (20% Training Data)

```bash
# AdamW with 20% data
python train.py --optimizer adamw --epochs 60 --exp A1_20p \
  --dataset-fraction 0.2

# FALCON with 20% data
python train.py --optimizer falcon --epochs 60 --exp F1_20p \
  --dataset-fraction 0.2 \
  --rank1-backend poweriter --mask-interval 5 --apply-stages "3,4" \
  --skip-mix-start 0.0 --skip-mix-end 0.7 \
  --retain-energy-start 0.90 --retain-energy-end 0.60
```

### Fixed-Time (10-Minute Budget)

```bash
# Stop all runs at 10 minutes
python train.py --optimizer adamw --epochs 60 --exp A1_t10 --time-budget-min 10
python train.py --optimizer muon --epochs 60 --exp M1_t10 --time-budget-min 10
python train.py --optimizer falcon --epochs 60 --exp F1_t10 --time-budget-min 10 \
  --rank1-backend poweriter --mask-interval 5 --apply-stages "3,4" \
  --skip-mix-start 0.0 --skip-mix-end 0.7 \
  --retain-energy-start 0.90 --retain-energy-end 0.60
```

## ğŸ”¬ Hyperparameter Tuning

Try different FALCON configurations:

```bash
# More aggressive filtering
python train.py --optimizer falcon --epochs 60 --exp F_aggressive \
  --retain-energy-start 0.95 --retain-energy-end 0.50

# Less skip-mix (more filtering throughout)
python train.py --optimizer falcon --epochs 60 --exp F_pure_filter \
  --skip-mix-start 0.0 --skip-mix-end 0.3

# Apply to more stages (slower but possibly better)
python train.py --optimizer falcon --epochs 60 --exp F_more_stages \
  --apply-stages "2,3,4"

# Higher rank approximation
python train.py --optimizer falcon --epochs 60 --exp F_rank2 \
  --rank-k 2

# Faster mask updates
python train.py --optimizer falcon --epochs 60 --exp F_freq_mask \
  --mask-interval 10
```

## ğŸ“ˆ View Results

After experiments complete:

```bash
# Generate all plots and tables
python scripts/plot_results.py

# View summary
cat results/results.md

# Check CSV table
column -t -s, results/table_summary.csv

# Open plots (if on local machine with GUI)
xdg-open results/fig_top1_vs_time.png
xdg-open results/fig_time_to_85.png
xdg-open results/fig_fixed_time_10min.png
xdg-open results/fig_robustness_noise.png
xdg-open results/fig_data_efficiency.png
```

## ğŸ› Troubleshooting

### Out of Memory
```bash
# Reduce batch size
python train.py --batch-size 64 ...

# Or disable caching temporarily
python train.py --mask-interval 1 ...
```

### Slow Performance
```bash
# Check GPU usage
nvidia-smi

# Reduce workers if CPU-bound
python train.py --workers 2 ...

# Use fast-mask approximation
python train.py --fast-mask ...
```

### Import Errors
```bash
# Check environment
conda activate falcon
pip install torch torchvision muon-optimizer matplotlib pandas seaborn

# Or create fresh env
conda create -n falcon python=3.10
conda activate falcon
pip install torch torchvision muon-optimizer matplotlib pandas seaborn
```

## ğŸ“Š Expected Results (60 Epochs)

Based on preliminary runs:

| Metric | AdamW | Muon | FALCON v2 |
|--------|-------|------|-----------|
| Best Val@1 | ~85.4% | ~87.1% ğŸ† | ~86-87% |
| Time/Epoch | ~5.4s | ~6.0s | ~6.6s âœ¨ |
| Time to 85% | ~8min | ~7min | ~8-9min |
| Speedup vs v1 | - | - | **2.2x!** |

FALCON v2 is **2.2x faster** than v1 (6.6s vs 14.5s/epoch) while maintaining competitive accuracy!

## ğŸ¯ Next Steps

1. Run full experiments: `bash scripts/run_v2.sh`
2. Generate plots: `python scripts/plot_results.py`
3. Read improvement guide: `FALCON_V2_SUMMARY.md`
4. Tune hyperparameters (see Section above)
5. Write paper! ğŸ“

---

**Questions?** Check `FALCON_V2_SUMMARY.md` for detailed documentation and improvement roadmap.
