\section{Results}
\label{sec:results}

\subsection{Full Training Performance}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/fig_top1_vs_time.png}
\caption{Validation accuracy vs wall-clock time. All three optimizers converge to $\sim$90\% accuracy, with Muon slightly ahead. FALCON matches final accuracy but requires more time due to FFT overhead.}
\label{fig:accuracy_time}
\end{figure}

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{lccc}
\toprule
\textbf{Optimizer} & \textbf{Accuracy} & \textbf{Time (min)} & \textbf{s/epoch} \\
\midrule
AdamW & 90.28\% & 5.00 & 4.8 \\
Muon & \textbf{90.49\%} & 5.37 & 5.3 \\
FALCON & 90.33\% & 6.99 & \textbf{6.7} \\
\bottomrule
\end{tabular}
\caption{Full training results (60 epochs, 100\% data). FALCON achieves competitive accuracy but with 40\% overhead.}
\label{tab:full_training}
\end{table}

\textbf{Key Observations:}
\begin{enumerate}
    \item \checkmark \textbf{Accuracy Parity:} FALCON within 0.16\% of Muon, 0.05\% above AdamW
    \item \ding{55} \textbf{40\% Slower:} 6.7s/epoch vs 4.8s/epoch for AdamW
    \item \ding{55} \textbf{28\% Lower Throughput:} 7,486 vs 10,382 images/sec
    \item \checkmark \textbf{Muon Best:} +0.21\% over AdamW with only +10\% overhead
\end{enumerate}

\textbf{Statistical Significance:} All three accuracies within $\pm$0.2\% (typical CIFAR-10 variance). Differences are not statistically significant with current sample size (single seed).

\subsection{Convergence Analysis}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/fig_time_to_85.png}
\caption{Time required to reach 85\% validation accuracy. Muon converges fastest (7\% faster than AdamW), while FALCON is 6\% slower despite requiring fewer epochs.}
\label{fig:convergence}
\end{figure}

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{lccc}
\toprule
\textbf{Optimizer} & \textbf{Time} & \textbf{Epochs} & \textbf{Speed} \\
\midrule
Muon & \textbf{1.18 min} & $\sim$13 & 1.08$\times$ \\
AdamW & 1.27 min & $\sim$15 & 1.0$\times$ \\
FALCON & 1.35 min & $\sim$10 & 0.94$\times$ \\
\bottomrule
\end{tabular}
\caption{Time to 85\% accuracy. Muon fastest.}
\label{tab:convergence}
\end{table}

\textbf{Analysis:} Muon converges fastest due to orthogonal updates providing stable directions. FALCON reaches 85\% in fewer epochs ($\sim$10 vs $\sim$15) but higher per-epoch cost makes wall-clock time 6\% slower than AdamW.

\subsection{Fixed-Time Performance}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/fig_fixed_time_10min.png}
\caption{Best accuracy achieved within 10-minute training budget. FALCON's per-epoch overhead significantly handicaps performance in time-constrained scenarios.}
\label{fig:fixed_time}
\end{figure}

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{lcc}
\toprule
\textbf{Optimizer} & \textbf{Accuracy} & \textbf{Epochs} \\
\midrule
AdamW & 90.28\% & 57 \\
Muon & \textbf{90.49\%} & 55 \\
FALCON & 87.77\% & 18 \\
\bottomrule
\end{tabular}
\caption{10-minute fixed budget. FALCON handicapped by overhead.}
\label{tab:fixed_time}
\end{table}

\textbf{Critical Finding:} FALCON's per-epoch overhead (40\%) significantly handicaps performance in time-constrained scenarios. Completes only 18/57 epochs (31.6\%) that AdamW does in same time.

\textbf{Implication:} FALCON not suitable for rapid prototyping or resource-limited settings.

\subsection{Data Efficiency}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/fig_data_efficiency.png}
\caption{Accuracy across different training data fractions. Contrary to hypothesis, FALCON shows no advantage with limited data, performing 0.8-1.0\% worse than AdamW.}
\label{fig:data_efficiency}
\end{figure}

\subsubsection{20\% Data (10k images, 60 epochs)}

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{lcc}
\toprule
\textbf{Optimizer} & \textbf{Accuracy} & \textbf{vs AdamW} \\
\midrule
AdamW & 80.66\% & --- \\
Muon & \textbf{80.78\%} & +0.12\% \\
FALCON & 79.89\% & -0.77\% \\
\bottomrule
\end{tabular}
\caption{20\% data (10k images). FALCON underperforms.}
\label{tab:data_20}
\end{table}

\subsubsection{10\% Data (5k images, 100 epochs)}

\begin{table}[h]
\centering
\footnotesize
\begin{tabular}{lcc}
\toprule
\textbf{Optimizer} & \textbf{Accuracy} & \textbf{vs AdamW} \\
\midrule
AdamW & \textbf{75.43\%} & --- \\
Muon & 75.37\% & -0.06\% \\
FALCON & 74.40\% & -1.03\% \\
\bottomrule
\end{tabular}
\caption{10\% data (5k images). FALCON gap worsens.}
\label{tab:data_10}
\end{table}

\textbf{Hypothesis Rejection:} We hypothesized frequency filtering would provide implicit regularization beneficial for limited data. Results show the opposite: FALCON performs 0.8-1.0\% worse than AdamW with limited data. Gap increases as data fraction decreases (0.77\% $\rightarrow$ 1.03\%).

\textbf{Possible Explanation:} As shown in Figure~\ref{fig:real_filtering}, our 50\% retention setting (late training) removes substantial semantic information---not just noise. With only 5k-10k training examples, every gradient component matters, and aggressive filtering likely discards signals crucial for learning from limited data.

\textbf{Muon Performance:} Maintains parity with AdamW (within 0.12\%), demonstrating robustness to sample size without hurting performance.

\subsection{Computational Breakdown}

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figures/fig_computational_breakdown.png}
\caption{(Left) Per-epoch time comparison showing FALCON's 40\% overhead. (Right) FALCON time breakdown revealing FFT operations consume $\sim$25\% of optimizer time.}
\label{fig:comp_breakdown}
\end{figure}

\begin{table}[h]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Component} & \textbf{Time (s)} & \textbf{\% of Total} \\
\midrule
FFT Forward & 0.4 & 13\% \\
Energy \& Mask & 0.3 & 9\% \\
Rank-k Approx & 0.5 & 16\% \\
FFT Inverse & 0.4 & 13\% \\
Muon Step & 0.5 & 16\% \\
AdamW Step & 0.3 & 9\% \\
EMA Update & 0.1 & 3\% \\
Other & 0.7 & 21\% \\
\midrule
\textbf{Total Optimizer} & \textbf{3.2} & \textbf{100\%} \\
\bottomrule
\end{tabular}
\caption{FALCON optimizer step breakdown. FFT operations (forward + inverse) consume 0.8s ($\sim$25\% of optimizer time), the primary source of overhead.}
\label{tab:breakdown}
\end{table}

\textbf{Key Insight:} FFT operations (forward + inverse) consume 0.8s per step ($\sim$25\% of optimizer time). This is the primary source of overhead. Rank-k approximation adds another 0.5s (16\%). Forward/backward passes (3.5s total) are unchanged across optimizers.
